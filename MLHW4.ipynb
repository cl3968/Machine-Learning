{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCI-UA.0473-â€‹001 Introduction to Machine Learning\n",
    "\n",
    "# Homework 4\n",
    "\n",
    "\n",
    "### Name: Christina Liu\n",
    "\n",
    "\n",
    "### Due: Nov. 18, 2019\n",
    "\n",
    "\n",
    "## Goal:  The goal of this homework is to practice implementing Gaussian Discriminant generative model as well as to explore connections between Bayesian inference and regularization in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian Discriminant Analysis\n",
    "\n",
    "Here we return to the breast cancer dataset. We will use Gaussian discriminant analysis to classify tumors as either malignant or benign.  \n",
    "\n",
    "### Part (a)\n",
    "Implement the 3 methods in the following class.  You may wish to refer to the following Scipy documentation for [multivariate_normal](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDA:\n",
    "    \n",
    "    \"\"\"\n",
    "    Create an instance of a Gaussian discriminant model.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_feat, train_targ):\n",
    "        assert train_feat.shape[0] == train_targ.shape[0]\n",
    "        self.N_train = train_feat.shape[0] # The number of training points.\n",
    "        self.d = train_feat.shape[1] # The dimension of the data.\n",
    "        self.train_feat = train_feat # Training features.\n",
    "        self.train_targ = train_targ # Training labels (either 0 or 1).\n",
    "    \n",
    "    \"\"\"\n",
    "    Learn the parameters for the mean and covariance of the two classes.  Sets the parameters of the Gaussian\n",
    "    Discriminant model.\n",
    "    \n",
    "    phi - float between 0 and 1.\n",
    "    m0 - numpy array of shape (d, ) for d >= 1, mean of data from class 0\n",
    "    m1 - numpy array of shape (d, ) for d >= 1, mean of data from class 1\n",
    "    C - numpy array of shape (d, d) for d >= 1, covariance of data\n",
    "    \n",
    "    Output:\n",
    "    None\n",
    "    \"\"\"\n",
    "    def fit(self):\n",
    "        X = self.train_feat\n",
    "        y = self.train_targ\n",
    "        N = self.N_train\n",
    "        d = self.d\n",
    "        \n",
    "        ## TO DO STARTS HERE ##\n",
    "        self.__classes = np.unique(y)\n",
    "        n_classes = len(self.__classes)\n",
    "        \n",
    "        self.__phi = np.zeros((n_classes, 1))\n",
    "        self.__means = np.zeros((n_classes, d))\n",
    "        self.__sigma = 0\n",
    "        for i in range(n_classes):\n",
    "            indexes = np.flatnonzero(y == self.__classes[i])\n",
    "\n",
    "            self.__phi[i] = len(indexes) / N\n",
    "            self.__means[i] = np.mean(X[indexes], axis=0)\n",
    "            self.__sigma += np.cov(X[indexes].T) * (len(indexes) - 1)\n",
    "\n",
    "        self.__sigma /= N\n",
    "        \n",
    "                \n",
    "        ## TO DO ENDS HERE ##\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the prediction on the test data.\n",
    "    \n",
    "    Input:\n",
    "    test_data - numpy array of shape (N_test, d) corresponding to the test data.\n",
    "    \n",
    "    Output:\n",
    "    y_pred - numpy array of shape (N_test, ) whose entries are either 0 or 1 corresponding to the predicted class.\n",
    "    \"\"\"\n",
    "    def predict(self, test_data):\n",
    "        \n",
    "        ## TO DO STARTS HERE ## \n",
    "        pdf = lambda mean: mvn.pdf(test_data, mean=mean, cov=self.__sigma)\n",
    "        y_probs = np.apply_along_axis(pdf, 1, self.__means) * self.__phi\n",
    "\n",
    "        y_pred= self.__classes[np.argmax(y_probs, axis=0)]\n",
    "    \n",
    "        ## TO DO ENDS HERE ##\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    \"\"\"\n",
    "    Draw samples from the conditional distribution p(x|y = label).\n",
    "    \n",
    "    Input:\n",
    "    label - int, either 0 or 1 corresponding to the class to sample from.\n",
    "    n_samples - int, the number of samples to draw with class label.\n",
    "    \n",
    "    Output:\n",
    "    x_sampled - numpy array of shape (d, ), a single sample from the conditional distribution.\n",
    "    \"\"\"\n",
    "    def sample(self, label, n_samples):\n",
    "       \n",
    "        ## TO DO STARTS HERE ## \n",
    "        mn = mvn(mean=self.__means[label], cov=self.__sigma)\n",
    "        x_sampled = mn.rvs(size=n_samples)\n",
    "        ## TO DO ENDS HERE ##\n",
    "        \n",
    "        return x_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Fit your GDA model to the cancer training dataset, evaluate it on the test set, and report the accuracy.  You should achieve an accuracy $\\sim 85 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the data.\n",
    "cancer_data = datasets.load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "# Splitting the data into a training and test set.\n",
    "N = X.shape[0]\n",
    "split_idx = int(0.7 * N)\n",
    "X_train = X[:split_idx, :2]\n",
    "X_test = X[split_idx:, :2]\n",
    "y_train = y[:split_idx]\n",
    "y_test = y[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.5%\n"
     ]
    }
   ],
   "source": [
    "## TO DO STARTS HERE ##\n",
    "model = GDA(X_train, y_train)\n",
    "model.fit()\n",
    "y_pred= model.predict(X_test)\n",
    "\n",
    "accuracy = len(y_test[y_pred==y_test])/len(y_test)\n",
    "## TO DO ENDS HERE ##\n",
    "\n",
    "print(\"Accuracy = {:0.1f}%\".format(100 * accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "One of the big advantages of generative models is that they can be used to create synthetic data.  This synthetic data can be used for training a classifier.  In particular, we can now generate balanced datasets.\n",
    "\n",
    "Use your trained model to generate a synthetic dataset with exactly 50 samples from each class (100 total).  Then create a scatter plot of the results.  Be sure to label the data points according to which class they are in.  You should use **cancer_data['target_names']** to get the list of the names of the two targets rather than refering to them as class 0 and class 1.  For more information about the cancer dataset you can look at **cancer_data.keys()**, although this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate and plot the new fake dataset, which is balanced with 50 data points from each class (100 total).\n",
    "\n",
    "## TO DO STARTS HERE ##\n",
    "\n",
    "\n",
    "x_sampled0 = model.sample(0, 50)\n",
    "x_sampled1 = model.sample(1, 50)\n",
    "\n",
    "\n",
    "plt.plot(x_sampled0[:,0],x_sampled0[:,1],  'o', color = 'blue', label = 'malignant')\n",
    "plt.plot(x_sampled1[:,0], x_sampled1[:,1], 'o', color = 'orange', label = 'benign')\n",
    "\n",
    "## TO DO ENDS HERE ##\n",
    "\n",
    "plt.xlabel(cancer_data['feature_names'][0])\n",
    "plt.ylabel(cancer_data['feature_names'][1])\n",
    "plt.title('GDA: Synthetic Breast Cancer Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)\n",
    "Consider the model\n",
    "$$\n",
    "y_i = w^Tx_i + \\epsilon_i\n",
    "$$\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$ are i.i.d.  Our goal is to infer the weight vector $w$ given $N$ data points $\\{(x_i, y_i)\\}_{i=1}^N$.\n",
    "Assume the prior distribution over $w$ is a multivariate Gaussian distribution $N(\\nu, \\Gamma)$.  \n",
    "\n",
    "\n",
    "Write down the posterior distribution (up to a normalizing constant) for $w$ given the $N$ training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**\n",
    "$$posterior = \\frac{likelihood*prior}{normalization}$$ \n",
    "\n",
    "Using Baye's rule, we get $$p(w|y,x) = \\frac{p(y|w,x)*p(w|x)}{p(y|x)} = \\frac{(\\prod^N_{i=1}p(x_i|w))*p(w|x)}{\\int \\prod^N_{i=1}p(x_i|w)*p(w|x)dw}$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "If we use only a single training point then we can obtain a posterior distribution $p_1(w | x_1,y_1)$.  Given a new training point we can use $p_1$ as a prior to obtain a new posterior $p_2$.  Repeat this for all the $N$ training points.  \n",
    "\n",
    "Will the result of updating sequentially be the same as updating the prior based on all $N$ training points at once?  Give a proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**\n",
    "The result of updating sequentially will not be the same as updating the prior based on all N training points at once. Updating the prior based on all N training points at once would result in what we got for part a, however, updating sequentially would result in revised = current x new likelihood instead of posterior= prior x likelihood. $$p_{n+1}(w_{n+1}|y,x_n) = p_{n}(y|w,x_n)*p_{n+1}(w_{n+1}|x_n)$$\n",
    "which is differnet from part a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "Recall that for linear regression we are minimizing a loss function\n",
    "$$\n",
    "L(w) = \\sum_{i=1}^N (w^T x_i - y_i)^2\n",
    "$$\n",
    "and that this corresponds to maximizing the log-likelihood $\\log p(y|X,w)$.  \n",
    "\n",
    "1.  Show that minimizing the regularized loss function\n",
    "$$\n",
    "L_{\\lambda}(w) = \\lambda \\|w\\|^2 + \\sum_{i=1}^N (w^T x_i - y_i)^2\n",
    "$$\n",
    "corresponds to finding the maximum (MAP estimator)\n",
    "$$\n",
    "w^* = \\underset{w}{\\mathrm{argmax}}\\ p(w | X,y)\n",
    "$$\n",
    "when the prior distribution for $w$ is an isotropic Gaussian (i.e. a Gaussian of the form $N(0, \\sigma^2 I)$, where $I$ is the identity matrix).  \n",
    "\n",
    "2.  What is the relation between $\\lambda$ and $\\sigma^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer goes here**\n",
    "1. $$\n",
    "min (L_{\\lambda}(w)) = min(\\lambda \\|w\\|^2 + \\sum_{i=1}^N (w^T x_i - y_i)^2)\n",
    "$$\n",
    "$$= \\underset{w}{\\mathrm{argmax}}(\\frac{-X*\\pi}{2}+\\frac{-y*\\pi}{2}-\\lambda \\|w\\|^2 + \\sum_{i=1}^N -(w^T x_i - y_i)^2))$$\n",
    "$$= \\underset{w}{\\mathrm{argmax}}(p(w)+p(X,y|w))$$\n",
    "$$= \\underset{w}{\\mathrm{argmax}}(p(w)+p(X,y|w)-p(X,y))$$\n",
    "$$\n",
    "w^* = \\underset{w}{\\mathrm{argmax}}\\ p(w | X,y)\n",
    "$$\n",
    "2. In linear regression, we have $L = \\frac{1}{2} \\|\\hat{y}-y\\| + \\frac{\\lambda}{2}  \\|w\\| \n",
    "$ where $y = w^{T}x$.When minimizing this objective function, we end up with $L= \\frac{1}{2} \\|\\hat{y}-y\\|$. When we have a Bayesian view of linear regression, the posterior is likelihood times prior: $p(w|x,y)=p(y|x,w)*p(w|\\mu_0, \\sigma^2)$. When we use the Gaussian prior for weight w, we get the following: $p(w|\\mu_0, \\sigma^2)=N(0,\\sigma^2 )$. SO the relationship of $\\lambda $ and $\\sigma^2$ is $\\lambda=\\frac{1}{2\\sigma^2}$. $\\lambda$ is the regularization rate and $\\sigma^2$ is the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d)\n",
    "\n",
    "Now you'll implement Bayesian linear regression to fit a high degree polynomial to some noisy data.  Assume that $x_0,\\ldots,x_{10}$ are equally spaced points on the interval $[0,1]$.  In other words, $x_i = i/10$ for $i=0,\\ldots,10$.  We wish to approximate some underlying function $f$ with prescibed values $f_i = f(x_i)$ by using a polynomial interpolation.  Recall that we can perfectly interpolate the points using a polynomial of degree $11$ or more.  However, if there is noise in our data this will give very poor results.  In particular, we assume that we instead observe\n",
    "$$\n",
    "y_i = f_i + \\epsilon_i\n",
    "$$\n",
    "where $\\epsilon_i$ are iid $N(0,\\gamma^2)$ random variables.  We can use a Bayesian approach to help fix this.  Here the parameter $\\theta \\in \\mathbb{R}^{11}$ will denote the coefficients of the polynomial.\n",
    "$$\n",
    "\\theta_0 + \\theta_1 x + \\cdots + \\theta_{10}x^{10} = (1,x,\\ldots,x^{10})^T \\theta\n",
    "$$\n",
    "Assume the prior distribution for $\\theta$ is an isotropic Gaussian of the form $N(0,\\sigma^2 I)$.\n",
    "\n",
    "Implement the two functions below which compute the MAP estimator for the parameters $\\theta$ and evaluates the corresponding polynomial.  Hint: It may be useful to consider the Vandermonde matrix $V \\in \\mathbb{R}^{11 \\times 11}$, whose entries are $V_{ij} = (x_i)^j$ for $0 \\le i,j, \\le 10$.  You may also want to use the Numpy linear algebra package (numpy.linalg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "y - numpy array of shape (11,) of the observed function values y_i = f_i + epsilon_i.\n",
    "gamma - positive float, standard deviation of observational noise\n",
    "sigma - positive float, standard deviation of the prior\n",
    "\n",
    "Output:\n",
    "theta - numpy array of shape (11,) of the fitted parameters\n",
    "\"\"\"\n",
    "def fit(y, gamma, sigma):\n",
    "    ## TO DO STARTS HERE ##\n",
    "    beta = 1/gamma**2\n",
    "    alpha = 1/sigma**2\n",
    "    theta = alpha + beta*np.transpose(y)*y\n",
    "    \n",
    "    ## TO DO ENDS HERE ##\n",
    "    return theta\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the polynomial with coefficients theta at the points x.\n",
    "\n",
    "Input:\n",
    "x - numpy array of shape (N, ) for N >= 1, the points to evaluate the polynomial at.\n",
    "theta - numpy array of shape (11, ), the coefficients of the polynomial\n",
    "\n",
    "theta[0] + theta[1]*x + ... + theta[9]*x^9 + theta[10]*x^10\n",
    "\n",
    "Output:\n",
    "vals - numpy array of shape (N, ), the evaluation of the polynomial at the points x.\n",
    "\"\"\"\n",
    "def evaluate(x, theta):\n",
    "    vals = np.zeros(x.shape)\n",
    "    \n",
    "    ## TO DO STARTS HERE ##\n",
    "    vals=vals*theta\n",
    "    vals = vals *np.transpose(x)\n",
    "    \n",
    "    ## TO DO ENDS HERE ##\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e)\n",
    "\n",
    "Choose an appropriate choice of $\\sigma$ so that the fitted polynomial closely matches the true function $f(x) = x$.  There is not a unique answer to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,) (11,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-9e6c519a812f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'Approximation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'$f(x)$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$x$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-322-03aaf82f2b78>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(x, theta)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m## TO DO STARTS HERE ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mvals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,) (11,) "
     ]
    }
   ],
   "source": [
    "## TO DO STARTS HERE ##\n",
    "# Choose a value of sigma.\n",
    "sigma = 1.0\n",
    "## TO DO ENDS HERE ##\n",
    "\n",
    "\n",
    "# Get the noisy observations and fit the polynomial.\n",
    "x = np.linspace(0, 1, 11)\n",
    "gamma = 0.25\n",
    "np.random.seed(1)\n",
    "y = x + gamma*np.random.randn(11)\n",
    "\n",
    "theta = fit(y, gamma = gamma, sigma = sigma)\n",
    "\n",
    "\n",
    "xx = np.linspace(0,1,1000)\n",
    "plt.plot(xx, evaluate(xx,theta), label = r'Approximation')\n",
    "plt.plot(xx, xx, 'k--', alpha = 0.7, label = r'$f(x)$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.title('Regularized Approximation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
